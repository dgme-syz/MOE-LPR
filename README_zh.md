# MOE-LPR

\[ [English](README.md) | ä¸­æ–‡ \]

âœ¨ è¿™æ˜¯è®ºæ–‡ [MoE-LPR: Multilingual Extension of Large Language Models through Mixture-of-Experts with Language Priors Routing](https://arxiv.org/abs/2408.11396) çš„ä¸€ä¸ªç²—ç³™å®ç°ï¼Œå…¶ä¸­å¤§é‡å‚è€ƒäº†åŸä½œè€…ä»“åº“çš„æ€è·¯ï¼Œç›®çš„æ˜¯ä¾¿äºè‡ªå·±å­¦ä¹ å’Œç†è§£å…¶ä½¿ç”¨æ–¹æ³•ã€‚

### ğŸš€åç½®é¢„è®­ç»ƒ (Post-Pretrain)

**é˜¶æ®µ1 (stage1)**

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œè®­ç»ƒæ–°å¢çš„ä¸“å®¶æ¨¡å—å’Œè·¯ç”±å™¨ã€‚

```bash
python -m train_moe \
    --model_name Qwen/Qwen1.5-0.5B \
    --topk 2 \
    --moe_num_experts 4 \
    --aux_loss_coef 0.01 \
    --lpr_loss_coef None \
    --dataset path_to_your_dataset \
    --val_size 0.01 \
    --cutoff_len 1024 \
    --batch_size 16 \
    --output_dir path_to_your_output_and_save \
    --lr_scheduler_type cosine \
    --save_total_limit 10 \
    --save_steps 1000 \
    --learning_rate 5e-5 \
    --num_train_epochs 1 \
    --bf16 False \
    --train_only_router False
```

**é˜¶æ®µ2 (stage2)**

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œåªéœ€è¦è®­ç»ƒè·¯ç”±å™¨ã€‚

```bash
python -m train_moe \
    --model_name Qwen/Qwen1.5-0.5B \
    --aux_loss_coef None \
    --lpr_loss_coef 0.1 \
    --dataset path_to_your_dataset \
    --val_size 0.01 \
    --cutoff_len 1024 \
    --batch_size 16 \
    --output_dir path_to_your_output_and_save \
    --lr_scheduler_type cosine \
    --save_total_limit 10 \
    --save_steps 1000 \
    --learning_rate 5e-5 \
    --num_train_epochs 1 \
    --bf16 False \
    --train_only_router True \
    --adapter_name your_peft_adapter_path \
    --max_samples 50000 
```

### **å¯èƒ½çš„ Bug**

1. å› ä¸º [PEFT](https://github.com/huggingface/peft) åŒ…è¿˜ä¸æ”¯æŒè‡ªå®šä¹‰ Adapterï¼Œæ‰€ä»¥æˆ‘åœ¨ `/moe/__init__.py` ä¸­ä½¿ç”¨äº†çŒ´å­è¡¥ä¸çš„æŠ€å·§ï¼Œå¯èƒ½å­˜åœ¨æ¨¡å—å‡½æ•°æ²¡æœ‰å®Œå…¨æ›¿æ¢å¹²å‡€çš„é—®é¢˜ã€‚
2. å¤§éƒ¨åˆ†ä»£ç é€»è¾‘ç›´æ¥æ¥æºäº[åŸä½œè€…](https://github.com/NJUNLP/MoE-LPR/tree/master)ã€‚æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œéœ€è¦å¯¹ Qwen2 æ¶æ„è¿›è¡Œä¸€äº›ä¿®æ”¹ï¼Œä»¥ä¾¿è®¡ç®— `balance_loss` å’Œ `lpr_loss`ï¼Œè¿™å¯èƒ½å¯¼è‡´æ½œåœ¨çš„é—®é¢˜ã€‚ä¿®æ”¹åçš„æ¨¡å‹ç»“æ„å¯ä»¥åœ¨ `qwen2.py` ä¸­æ‰¾åˆ°ã€‚

### ğŸ¨è¯„ä¼° (Evaluation)

æˆ‘ä½¿ç”¨äº†æ–¹ä¾¿çš„ [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)ï¼Œå¹¶å°†å…¶ä»“åº“æºç æ‹·è´ä¸‹æ¥ï¼Œå› ä¸ºæˆ‘éœ€è¦ç›´æ¥å¯¼å…¥æ¨¡å‹ï¼Œè€Œä¸æ˜¯é€šè¿‡å®ƒçš„ä»£ç åŠ è½½æ¨¡å‹ã€‚

æ ¹æ®ä»¥ä¸‹è„šæœ¬ä»¥åŠåŸå§‹ä»“åº“çš„æ–‡æ¡£ï¼Œä½ å¯ä»¥ä½¿ç”¨è®ºæ–‡ä¸­æåˆ°çš„æ¨¡å‹æµ‹è¯•å„ç§åŸºå‡† (benchmarks)ã€‚

```bash
#!/bin/bash

BASE_MODEL_PATH="Qwen/Qwen1.5-0.5B"
PEFT_MODEL_PATH="./output/checkpoint-1"
OUTPUT_PATH="./eval_output"
BATCH_SIZE=1

export WANDB_DISABLED=true

python -m eval_moe --model hf \
    --model_args pretrained=$BASE_MODEL_PATH,peft=$PEFT_MODEL_PATH,dtype="bfloat16" \
    --tasks hellaswag \
    --device cuda:0 \
    --num_fewshot 10 \
    --output_path $OUTPUT_PATH \
    --batch_size $BATCH_SIZE
```

> `eval.bat` æ˜¯ä¸º Windows ç”¨æˆ·æä¾›çš„ç‰ˆæœ¬ã€‚